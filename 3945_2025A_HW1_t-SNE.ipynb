{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10316d5779a3733",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercise 1: t-SNE\n",
    "\n",
    "## Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "\n",
    "* The homework assignments are executed automatically. \n",
    "* Failure to comply with the following instructions will result in a significant penalty. \n",
    "* Appeals regarding your failure to read these instructions will be denied. \n",
    "* Kind reminder: the homework assignments contribute 60% of the final grade.\n",
    "\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This Jupyter notebook contains all the step-by-step instructions needed for this exercise.\n",
    "1. Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise may take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit to this jupyter notebook. Tests will not be graded nor checked.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/).\n",
    "1. Your code must run without errors. Use at least `numpy` 1.15.4. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Submission includes a zip file that contains this notebook, with your ID as the file name. For example, `hw1_123456789_987654321.zip` if you submitted in pairs and `hw1_123456789.zip` if you submitted the exercise alone. The name of the notebook should follow the same structure.\n",
    "   \n",
    "Please use only a **zip** file in your submission.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Please sign that you have read and understood the instructions: \n",
    "\n",
    "316492776\n",
    "318900016\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a25c0c-b60f-4320-8555-63c2a28c8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required dependencies\n",
    "!pip3 install numpy scipy pandas scikit-learn matplotlib mpl-tools tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735832cbfa43f83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877cca-f7d1-41ca-9d3d-28d587bec85c",
   "metadata": {},
   "source": [
    "# Design your algorithm\n",
    "Make sure to describe the algorithm, its limitations, and describe use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7760b-b4ef-45a9-9aad-88b45fd8d442",
   "metadata": {},
   "source": [
    "tSNE is a non-linear dimensionality reduction algorithm, used mostly for the purpose of visualization in 2d and sometimes 3d.\n",
    "Its input is $n$ points in a high dimentional space $\\mathbb{R}^h$, and its output is $n$ correspoinding points in a $\\mathbb{R}^2$.\n",
    "\n",
    "The algorithm calculates pairwise similarity is the original space $p_{ij}$, and tries to make the similarities in the lower dimensional space $q_{ij}$ have the same distribution.\n",
    "Its cost function is KL-divergence between P and Q distributions, which means higher $p_{ij}$ values are more important than smaller ones, meaning tSNE generally focuses on preserving local structure but will sometimes fail to capture global relationships.\n",
    "\n",
    "tSNE is symetric (unlike regular SNE), and also uses t-distribution in the lower dimentional space in order to avoid shrinkage to a single point or a very dense cluster (since it has a heavy tail, unlike a gaussian which does not)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158ff2629daf2bb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Your implementations\n",
    "You may add new cells, write helper functions or test code as you see fit.\n",
    "Please use the cell below and include a description of your implementation.\n",
    "Explain code design consideration, algorithmic choices and any other details you think is relevant to understanding your implementation.\n",
    "Failing to explain your code will lead to point deductions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe206c9-d1f4-440b-aca0-c807cdd79451",
   "metadata": {},
   "source": [
    "We implement t-SNE as seen in the class, more specifically according to slides 61-66.\n",
    "The calculation of $q_{ij}$ and $p_{ij}$ is pretty straight forward according to the theorey shown in class. We did, however, add one major improvement to make the algorithm converge in feasable time. Instead of calculating $p_{ij}$ for all $i \\neq j$ pairs, we calculate it only for the $k$ closest neighbors (in the original space) of each point, since they contibute the most to the value of $C$, since if $X_i$ and $X_j$ are far apart, $p_{ij}$ will be closer to 0, and hence will not make much of a difference in the total summation of $C$.\n",
    "\n",
    "Everything is done with vectorized code, to maximize efficiency and speed as much as possible.\n",
    "\n",
    "By doing so, instead of calculating $N^2$ for each iteration for a total of $O(N^2 \\cdot i)$ with $i$ being the number of iteration, we calculate only $O(N^2 + N \\cdot k \\cdot i)$, and with $k \\ll N$ this makes a major difference.\n",
    "\n",
    "The optimization is done by gradient descent process, with an option for early stopping if no progress is made after some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85d8f7447ebce0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CustomTSNE:\n",
    "    def __init__(self, perplexity: float = 30.0, n_components: int = 2, n_iter: int = 1000, learning_rate: float = 200.0,\n",
    "                 n_iter_without_progress: int = 3000, verbose: bool = True):\n",
    "        self.perplexity = perplexity\n",
    "        self.n_components = n_components\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        # Note: You may add more attributes\n",
    "        self.sigma = 1  # TODO\n",
    "        self.n_iter_without_progress = n_iter_without_progress\n",
    "        self.verbose = verbose\n",
    "        self.min_iter = n_iter // 10\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        # Part 1: Implementing t-SNE\n",
    "\n",
    "        # Step 1: Compute pairwise affinities in the original space with a Gaussian distribution\n",
    "        # Your code here\n",
    "        X = X/np.max(X)\n",
    "        N = len(X)\n",
    "        n_neighbors = min(N-1, 3*int(self.perplexity))\n",
    "        X_dists = squareform(pdist(X))\n",
    "        n_nearest_indexes = np.argsort(X_dists, axis=1)[:, 1:n_neighbors+1]\n",
    "        # +1 to remove i=j\n",
    "        X_dists = np.take_along_axis(X_dists, n_nearest_indexes, axis=1)\n",
    "        p_ij = self.calc_pij(X_dists)\n",
    "\n",
    "        Y = np.random.random((N, self.n_components))\n",
    "\n",
    "        best_i, best_C = -1, np.inf\n",
    "        for i in tqdm(range(self.n_iter), disable=not self.verbose, desc=\"iteration\"):\n",
    "            Y_diff = Y[:, np.newaxis, :] - Y[n_nearest_indexes]\n",
    "            Y_dists = np.linalg.norm(Y_diff, axis=-1)\n",
    "            Y_dists = 1+Y_dists**2\n",
    "            Y_dists = Y_dists ** -1\n",
    "            q_ij = Y_dists / np.sum(Y_dists)  # normalize to a probabilty\n",
    "\n",
    "            C = p_ij * np.log(p_ij/q_ij)\n",
    "            C = np.sum(C)\n",
    "            if self.verbose:\n",
    "                print(f\"After {i} iterations, {C=}\")\n",
    "            if C < best_C:\n",
    "                best_i, best_C = i, C\n",
    "            if self.min_iter < i and best_i + self.n_iter_without_progress < i:\n",
    "                if self.verbose:\n",
    "                    print(\n",
    "                        f\"C has not improved for {self.n_iter_without_progress} steps. Stopping early.\")\n",
    "                return Y\n",
    "\n",
    "            grad_C_yi = 4 * \\\n",
    "                np.sum((p_ij - q_ij)[..., np.newaxis] *\n",
    "                       Y_diff * Y_dists[..., np.newaxis], axis=1)\n",
    "\n",
    "            # Return Y, the 2D representation of the input data\n",
    "            Y -= self.learning_rate * grad_C_yi\n",
    "        if self.verbose:\n",
    "            print(f\"{C=} after {self.n_iter} iterations\")\n",
    "        return Y\n",
    "\n",
    "    # Part 2: Transformation of New Data Points\n",
    "    def transform(self, X_original, Y_original, X_new):\n",
    "        N = len(X_new)\n",
    "        n_neighbors = min(N-1, 3*int(self.perplexity))\n",
    "        Y_new = []\n",
    "\n",
    "        # Added weights to the distances as following: [1, 1/2, 1/3, 1/4.... 1/n]\n",
    "        # The minimal distance get's the highest weight, and the weight is reduced as the distance increasing\n",
    "        W_arr = np.arange(1, n_neighbors + 1, dtype=float) ** -1\n",
    "\n",
    "        dists = cdist(X_new, X_original)\n",
    "        n_nearest_indexes = np.argsort(dists, axis=1)[:, :n_neighbors]\n",
    "        Y_new = np.average(\n",
    "            Y_original[n_nearest_indexes], weights=W_arr, axis=1)\n",
    "\n",
    "        # for i in range(X_new.shape[0]):\n",
    "        #     # Calculate the distance array for each one of the test values\n",
    "        #     d = cdist(X_new[i].reshape(1, -1), X_original)\n",
    "\n",
    "        #     # Sort the indices of the distances from minimal to maximal while limit just to the n_neighbors distances\n",
    "        #     d_argsort = np.argsort(d, axis=1).flatten()[:n_neighbors]\n",
    "\n",
    "        #     # Find the average location based on the indices of the distances, while the distances are weighted (Minimal distance -> max weight)\n",
    "        #     dest_value = np.average(Y_original[d_argsort], axis=0, weights=W_arr)\n",
    "\n",
    "        #     Y_new.append(dest_value)\n",
    "\n",
    "        return np.array(Y_new)\n",
    "\n",
    "    def calc_pij(self, X_dists, min_sigma: float = 1e-3, max_sigma: float = 1e+3):\n",
    "        N = len(X_dists)\n",
    "        min_sigma = np.ones((N, 1))*min_sigma\n",
    "        max_sigma = np.ones((N, 1))*max_sigma\n",
    "        for i in tqdm(range(30), disable=not self.verbose, desc=\"finding sigma\"):\n",
    "            curr_sigma = (min_sigma + max_sigma)/2\n",
    "            p_ij = -X_dists**2 / (2*curr_sigma**2)\n",
    "            # for numeric stability\n",
    "            p_ij -= p_ij.max(axis=1, keepdims=True)\n",
    "            p_ij = np.exp(p_ij)\n",
    "            # normalize to a probabilty\n",
    "            p_ij = p_ij / p_ij.sum(axis=1, keepdims=True)\n",
    "            distribution_entropy = (-p_ij * np.log2(p_ij)\n",
    "                                    ).sum(axis=1, keepdims=True)\n",
    "            distribution_perplexity = 2**distribution_entropy\n",
    "            min_sigma, max_sigma = (\n",
    "                np.where(distribution_perplexity <\n",
    "                         self.perplexity, curr_sigma, min_sigma),\n",
    "                np.where(distribution_perplexity >\n",
    "                         self.perplexity, curr_sigma, max_sigma)\n",
    "            )\n",
    "\n",
    "        return p_ij / p_ij.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24f179351fa008",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load data\n",
    "Please use the cell below to discuss your dataset choice and why it is appropriate (or not) for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4083f-5267-44d3-89ed-65864f82aa57",
   "metadata": {},
   "source": [
    "We chose to use MNIST dataset to our algorithm.\n",
    "MNIST is a very popular yet simple dataset, which is used wordlwide for basic/begginer ML tasks, and has many examples specifically for dimensionality reduction outputs (PCA and tSNE). This will allow us to compare ourself. \n",
    "It's usage is about encoding a digit in an image (Assuming the digit is shown as pixels just like one takes a photo of a car plate), and each image is labeled by it's digit value\n",
    "\n",
    "Using T-SNE algorithm, we can find the relationship and the distances between the images with a very clear and visual way, while since we have just 10 digits in MNIST (0-9), we can classify the images into 10 groups and show in 2D dimension the relationship and distance between them\n",
    "\n",
    "We split the dataset into train (80%) and test (20%) while the dataset contains 10,000 examples in total\n",
    "\n",
    "Since the image values are described by RGB codes (0 -> white to 255 -> black), the digits should be represented as black while the background as white, but in fact the digits are in various range of colors between white and black (gray, dark gray. etc). Then, in order to reduce errors since the colors distribution, we want to normalize it so the digit will be represented with one single RGB code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a3b8890e86f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Loading the data into train / test datasets\n",
    "\n",
    "dataset = pd.read_csv(\"./Dataset/mnist_dataset.csv\")\n",
    "\n",
    "# The X axis is the pixels RGB matrix that represents the digit\n",
    "X = dataset.loc[:,'1x1':].values\n",
    "\n",
    "# The y axis is the actual digit that the the pixels matrix represents\n",
    "y = dataset.loc[:,'label'].values\n",
    "\n",
    "X_train, X_test, label_train, label_test = train_test_split(\n",
    "    X, y, test_size=0.2)\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "X_train_normalized_opt1 = X_train / 255\n",
    "X_test_normalized_opt1 = X_test / 255\n",
    "\n",
    "# Normalizing the RGB codes by standard scaler\n",
    "sc = StandardScaler()\n",
    "X_train_normalized_opt2 = sc.fit_transform(X_train)\n",
    "X_test_normalized_opt2 = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061fa218-e946-419f-af5f-c26a204b669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14321076-8d5c-4f75-9eec-a123d4802b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda1548-1b9f-4b90-92e5-2675c8a0444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized_opt1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377574fd-827f-4685-a046-e2f40a078a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a ploting of sample train data\n",
    "\n",
    "fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train_normalized_opt1[i].reshape(28, 28),\n",
    "              cmap='binary', interpolation='nearest',\n",
    "              clim=(0, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b97ea-6e25-4620-9db0-3e150749729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a ploting of sample train data\n",
    "\n",
    "fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train_normalized_opt2[i].reshape(28, 28),\n",
    "              cmap='binary', interpolation='nearest',\n",
    "              clim=(0, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49bb42f79a55f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# t-SNE demonstration \n",
    "Demonstrate your t-SNE implementation.\n",
    "\n",
    "Add plots and figures. The code below is just to help you get started, and should not be your final submission.\n",
    "\n",
    "Please use the cell below to describe your results and tests.\n",
    "\n",
    "Describe the difference between your implementation and the sklearn implementation. Hint: you can look at the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064afb5-aeea-48d8-b315-921bf4f8238f",
   "metadata": {},
   "source": [
    "As we can see below, we've tested the module with a various combination of Hypher-Parameters\n",
    "\n",
    "The most important hypher-param is the perplexity. We found that a large number of perplexity causes a very dense and unified groups of digits but with some little mistakes since with high perplexity we can also \"catch\" neighbors that represents another disits. However, with a low number of perplexity the observabilities will not be unified and scattered along the graph\n",
    "Therefore, according to our research the optimal perplexity number is something about 15-30\n",
    "\n",
    "Regarding learning rate, higher learning rate causes the group be closer to each other, and smaller learning rate makes the groups be more far away, since the Y output values are getting more centralized with higher learning rate value. The optimal learning rate is about 100-200. However, it has less affect on the results than the perplexity\n",
    "\n",
    "Redarding the number of iterations, we've tested with 1000 and 500 iterations and didn't find meaningful changes. However a large number of iterations can cause overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3628856e1335fd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hypher_pharams_options = [{\n",
    "    'n_components': 2,\n",
    "    'n_iter': 1000,\n",
    "    'perplexity': 100,\n",
    "    'learning_rate': 200\n",
    "},{\n",
    "    'n_components': 2,\n",
    "    'n_iter': 1000,\n",
    "    'perplexity': 50,\n",
    "    'learning_rate': 200\n",
    "},{\n",
    "    'n_components': 2,\n",
    "    'n_iter': 500,\n",
    "    'perplexity': 30,\n",
    "    'learning_rate': 100\n",
    "},{\n",
    "    'n_components': 2,\n",
    "    'n_iter': 1000,\n",
    "    'perplexity': 15,\n",
    "    'learning_rate': 100\n",
    "},{\n",
    "    'n_components': 2,\n",
    "    'n_iter': 1000,\n",
    "    'perplexity': 15,\n",
    "    'learning_rate': 50\n",
    "},\n",
    "{\n",
    "    'n_components': 2,\n",
    "    'n_iter': 1000,\n",
    "    'perplexity': 5,\n",
    "    'learning_rate': 50\n",
    "},\n",
    "{\n",
    "    'n_components': 2,\n",
    "    'n_iter': 1000,\n",
    "    'perplexity': 50,\n",
    "    'learning_rate': 200\n",
    "},{\n",
    "    'n_components': 3,\n",
    "    'n_iter': 1000,\n",
    "    'perplexity': 20,\n",
    "    'learning_rate': 200\n",
    "}]\n",
    "\n",
    "results_array = []\n",
    "\n",
    "for hypher_params in hypher_pharams_options:\n",
    "    n_components = hypher_params[\"n_components\"]\n",
    "    n_iter = hypher_params[\"n_iter\"]\n",
    "    perplexity = hypher_params[\"perplexity\"]\n",
    "    learning_rate = hypher_params[\"learning_rate\"]\n",
    "    print(f\"Running with number of components {n_components}, number of iterations {n_iter}, perplexity {perplexity} and lr {learning_rate}\")\n",
    "    \n",
    "    # Run your custom t-SNE implementation\n",
    "    custom_tsne = CustomTSNE(n_components=n_components, n_iter=n_iter, perplexity=perplexity, learning_rate=learning_rate)\n",
    "    custom_Y = custom_tsne.fit_transform(X_train_normalized_opt1)\n",
    "    \n",
    "    # Run sklearn t-SNE\n",
    "    sk_tsne = TSNE(n_components=n_components, init='random', perplexity=perplexity)\n",
    "    sk_Y = sk_tsne.fit_transform(X_train_normalized_opt1)\n",
    "    results_array.append({'custom_Y': custom_Y, 'sk_Y': sk_Y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd6fc4-a435-4e9d-b1bf-5d4015d7030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, result in enumerate(results_array):\n",
    "    if result[\"sk_Y\"].shape[1] == 2:\n",
    "        # Visualization of the result\n",
    "        plt.figure()\n",
    "        plt.scatter(result[\"custom_Y\"][:, 0], result[\"custom_Y\"][:, 1], s=5, c=label_train.astype(int), cmap='tab10')\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"MNIST Data Embedded into 2D with Custom t-SNE with {hypher_pharams_options[idx]['n_iter']} iterations, perplexity {hypher_pharams_options[idx]['perplexity']} and lr {hypher_pharams_options[idx]['learning_rate']}\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.scatter(result[\"sk_Y\"][:, 0], result[\"sk_Y\"][:, 1], s=5, c=label_train.astype(int), cmap='tab10')\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"MNIST Data Embedded into 2D with Sklearn t-SNE with {hypher_pharams_options[idx]['n_iter']} iterations, perplexity {hypher_pharams_options[idx]['perplexity']} and lr {hypher_pharams_options[idx]['learning_rate']}\")\n",
    "        plt.show()\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(result[\"custom_Y\"][:, 0], result[\"custom_Y\"][:, 1], result[\"custom_Y\"][:, 2], s= 5, c=label_train, cmap='tab10')\n",
    "        plt.title(f\"MNIST Data Embedded into 3D with Custom t-SNE with {hypher_pharams_options[idx]['n_iter']} iterations, perplexity {hypher_pharams_options[idx]['perplexity']} and lr {hypher_pharams_options[idx]['learning_rate']}\")\n",
    "        plt.show()\n",
    "        plt.show()\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(result[\"sk_Y\"][:, 0], result[\"sk_Y\"][:, 1], result[\"sk_Y\"][:, 2], s= 5, c=label_train, cmap='tab10')\n",
    "        plt.title(f\"MNIST Data Embedded into 3D with Sklearn t-SNE with {hypher_pharams_options[idx]['n_iter']} iterations, perplexity {hypher_pharams_options[idx]['perplexity']} and lr {hypher_pharams_options[idx]['learning_rate']}\")\n",
    "        plt.show()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8f79d-ee3f-4020-96d9-a108bc419fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sklearn t-SNE\n",
    "sk_tsne_3d = TSNE(n_components=3, init='random', perplexity=N/10)\n",
    "sk_Y_3d = sk_tsne_3d.fit_transform(X_train_normalized_opt2)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(sk_Y_3d[:, 0], sk_Y_3d[:, 1],sk_Y_3d[:,2], s= 5, c=label_train, cmap='Spectral')\n",
    "plt.title('Visualizing Kannada MNIST through t-SNE in 3D', fontsize=24);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa2fceedc77e92",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# t-SNE extension - mapping new samples\n",
    "Demonstrate your t-SNE transformation procedure.\n",
    "\n",
    "Add plots and figures.\n",
    "\n",
    "Please use the cell below t describe your suggested approach in detail. Use formal notations where appropriate.\n",
    "Describe and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493f09c-41a0-416e-935d-eddd6673959a",
   "metadata": {},
   "source": [
    "We chose to use the transfort method as following:\n",
    "1. For each one of the test set observabilities, we've calculated the distance from it's RGB vector to any of the RGB vectors in the train set\n",
    "2. We find the top n nearest vectors (While n is the perplexity used in the fit_transofrm method) and we build a new matrix of (n X 2) that contains the original Y values of the n nearest vectors from the train set, sorted from 1st nearest to n nearest (Each Y value contains 2 values, the x axis and the y axis in the target 2D graph)\n",
    "3. The target Y value of the test RGB vector is the weighted average of the Y values of the n nearest vectors from the train set. While the weight of each value is $\\frac{1}{i}, 1 \\leq i \\leq n$ (The nearest value is weighted by $1$, the 2nd by $\\frac{1}{2}$, the 3rd by $\\frac{1}{3}$, and the n by $\\frac{1}{n}$)\n",
    "4. We repeat this for each one of the test values, until we build a final Y vector of (Test_Size X 2) of 2D values to be in the graph\n",
    "\n",
    "The reason we compare the distances between the RGB vectors of each one of the test examples to the train examples, is the assumption that each digit represented by RGB code is not exactly the same as other RGB code vectors of the same digit, but the distance is minimal to RGB vectors represents the same digit.\n",
    "Then we assume that most of the n RGB nearest vectors represents the same digit as the test sample\n",
    "Then we want to locate it around the n RGB nearest vectors but with more weight to the closest vector and less weight as the vector is getting less close\n",
    "\n",
    "With that approach we set the Y vectors of the test samples around the group of their digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38dc132b23e7b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for idx, result in enumerate(results_array):\n",
    "    # Transform new data\n",
    "    custom_Y_new = custom_tsne.transform(X_train_normalized_opt1,result[\"custom_Y\"],X_test)\n",
    "    \n",
    "    if result[\"custom_Y\"].shape[1] == 2:\n",
    "        # Visualization of the result\n",
    "        plt.figure()\n",
    "        plt.scatter(result[\"custom_Y\"][:, 0], result[\"custom_Y\"][:, 1], s=5, c=label_train.astype(int), cmap='tab10')\n",
    "        plt.scatter(custom_Y_new[:, 0], custom_Y_new[:, 1], marker = '*', s=50, linewidths=0.5, edgecolors='k', c=label_test.astype(int), cmap='tab10')\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"MNIST Data Embedded into 2D with Custom t-SNE with {hypher_pharams_options[idx]['n_iter']} iterations, perplexity {hypher_pharams_options[idx]['perplexity']} and lr {hypher_pharams_options[idx]['learning_rate']}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c95c7f-d3a9-4e3d-b539-02e020358766",
   "metadata": {},
   "source": [
    "# Use of generative AI\n",
    "Please use the cell below to describe your use of generative AI in this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36753fd7-8b2d-487b-82ae-dc318eca3ca6",
   "metadata": {},
   "source": [
    "We used ChatGPT to help us utilize numpy vectorised functions more effieintly. It also proposed subtracting the maximum when calculating exponents, in order to avoid very large/small values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee67dc6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
