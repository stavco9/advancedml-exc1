{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10316d5779a3733",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercise 1: t-SNE\n",
    "\n",
    "## Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "\n",
    "* The homework assignments are executed automatically. \n",
    "* Failure to comply with the following instructions will result in a significant penalty. \n",
    "* Appeals regarding your failure to read these instructions will be denied. \n",
    "* Kind reminder: the homework assignments contribute 60% of the final grade.\n",
    "\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This Jupyter notebook contains all the step-by-step instructions needed for this exercise.\n",
    "1. Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise may take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit to this jupyter notebook. Tests will not be graded nor checked.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/).\n",
    "1. Your code must run without errors. Use at least `numpy` 1.15.4. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Submission includes a zip file that contains this notebook, with your ID as the file name. For example, `hw1_123456789_987654321.zip` if you submitted in pairs and `hw1_123456789.zip` if you submitted the exercise alone. The name of the notebook should follow the same structure.\n",
    "   \n",
    "Please use only a **zip** file in your submission.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Please sign that you have read and understood the instructions: \n",
    "\n",
    "316492776\n",
    "318900016\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a25c0c-b60f-4320-8555-63c2a28c8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required dependencies\n",
    "!pip3 install numpy scipy pandas scikit-learn matplotlib mpl-tools tqdm seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735832cbfa43f83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sns.set_theme()\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877cca-f7d1-41ca-9d3d-28d587bec85c",
   "metadata": {},
   "source": [
    "# Design your algorithm\n",
    "Make sure to describe the algorithm, its limitations, and describe use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a3e07-a30d-4cca-9003-dcb3425bde9c",
   "metadata": {},
   "source": [
    "tSNE is a non-linear dimensionality reduction algorithm, used mostly for the purpose of visualization in 2d and sometimes 3d.\n",
    "Its input is $n$ points in a high dimentional space $\\mathbb{R}^h$, and its output is $n$ correspoinding points in a $\\mathbb{R}^2$ (or $\\mathbb{R}^3$).\n",
    "\n",
    "The algorithm calculates pairwise distances is the original space, tranlates them to a \"similarity\" probabilty distribution $p_{ij}$ using gaussians, and tries to make the similarities in the lower dimensional space $q_{ij}$ have that same distribution (this time with T-distribution - more about that below).\n",
    "\n",
    "Its cost function is KL-divergence between P and Q, which represents \"distance\" between distribution. It is minimized using gradient decsent.\n",
    "\n",
    "KL-devergence defenition, $\\sum_{i,j=1}^{N}{p_{ij} \\cdot \\log{\\frac{p_{ij}}{q_{ij}}}}$, causes higher $p_{ij}$ values to be more important than smaller ones, meaning tSNE generally focuses on preserving local structure (=higher $p_{ij}$ values corresponding to closer points in the original space) but sometimes fail to capture global relationships.\n",
    "\n",
    "This property, however convinient for visualization, means that tSNE is generally not a good choice to be used as a more \"mathematical\" option for dimensionality reduction, since it does not preserve basic geometric features of the data in the original space.\n",
    "\n",
    "Unlike regular SNE, tSNE is symetric, and uses T-distribution in the lower dimentional space in order to avoid shrinkage to a single point or a very dense cluster (since T-distribution has a heavy tail, unlike a gaussian which does not).\n",
    "\n",
    "The (important) hyperparameters:\n",
    "- n_components: the low dimension (almost always 2 or 3)\n",
    "- perplexity: relates to the amount of neighbors to be considered when calculating $\\sigma_i$\n",
    "- n_iter: the maximal number of iterations of the gradient descent process.\n",
    "- learning rate: learning rate for the gradient descent algorithm. High learning rate leads to faster loss decrease but may also fail to converge at later iterations. Low learning rates suffer less from that, but convergence more slowly and require more steps (and time) to reach small loss values. SKLearn implements \"auto\" method, which changes lr over time.\n",
    "- method: SKLearn supports the 'barnes_hut' distance approximations, which causes $q_{ij}$ to be $O(n \\log{n})$ rather than $O(n^2)$. It is less precise, but makes the algorithm way more scalable. Exact calculations become impractical with thousands of examples since $O(n^2)$ increases fast, while $O(n \\log{n})$ still performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158ff2629daf2bb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Your implementations\n",
    "You may add new cells, write helper functions or test code as you see fit.\n",
    "Please use the cell below and include a description of your implementation.\n",
    "Explain code design consideration, algorithmic choices and any other details you think is relevant to understanding your implementation.\n",
    "Failing to explain your code will lead to point deductions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe206c9-d1f4-440b-aca0-c807cdd79451",
   "metadata": {},
   "source": [
    "We implement t-SNE as seen in the class, more specifically according to slides 61-66.\n",
    "The calculation of $q_{ij}$ and $p_{ij}$ is pretty straight forward according to the theorey shown in class.\n",
    "\n",
    "We implemented a search to find $\\sigma_i$ for each example, as suggested in the piazza forum. For each example, we choose $\\sigma_i$ that achieves the desired perplexity. We implemented this by using a vectorized binary search (for all $\\sigma_i$ together), increasing or descreasing sigma to get closer to the desired perplexity. Since we cut the search space by half each iteration, the process converges fast.\n",
    "The (configurable) default sigma values are between $10^{-3}$ and $10^3$ with precision of $10^{-4}$, which covers more than enough values (when we ran the algorithm, final sigma values were all between $10^{-1}$ and $10^1$)\n",
    "\n",
    "We did not implement 'barnes_hut' optimization and we stick to the exact method since, meaning we don't scale so well for large numbers of examples. We did, however, make sure to write vectorized code to maximize efficiency and speed as much as possible, so we handle thousands of examples with little effort. \n",
    "\n",
    "We also did not implement lr decay methods, since using relatively low values and increasing the max steps seemed to do the job (graphs later), and did not take too much time to run.\n",
    "\n",
    "The optimization is done by gradient descent process, with an option for early stopping if no progress is made after some time.\n",
    "\n",
    "We added few more parameters to the original signature:\n",
    "- n_iter_without_progress: number of iterations to wait before early stopping if the loss hasn't decreased.\n",
    "- verbose: verbosity, used to help us debug (helpful prints and progress bars)\n",
    "- min_sigma & max_sigma: control the range of values that $\\sigma_i$ can get\n",
    "- transform_n_neighbors: the number of neighbors from the train to be considered when calculating the transform method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85d8f7447ebce0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CustomTSNE:\n",
    "    def __init__(\n",
    "        self,\n",
    "        perplexity: float = 30.0,\n",
    "        n_components: int = 2,\n",
    "        n_iter: int = 1000,\n",
    "        learning_rate: float = 200.0,\n",
    "        n_iter_without_progress: int = 300,\n",
    "        verbose: bool = False,\n",
    "        min_sigma: float = 1e-3,\n",
    "        max_sigma: float = 1e3,\n",
    "        transform_n_neighbors: int = 10,\n",
    "    ):\n",
    "        self.perplexity = perplexity\n",
    "        self.n_components = n_components\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        # Note: You may add more attributes\n",
    "        self.n_iter_without_progress = n_iter_without_progress\n",
    "        self.verbose = verbose\n",
    "        self.min_sigma = min_sigma\n",
    "        self.max_sigma = max_sigma\n",
    "        self.transform_n_neighbors = transform_n_neighbors\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        # Part 1: Implementing t-SNE\n",
    "\n",
    "        # Step 1: Compute pairwise affinities in the original space with a Gaussian distribution\n",
    "        # Your code here\n",
    "        X = X / np.max(X)\n",
    "        N = len(X)\n",
    "        X_dists = squareform(pdist(X))\n",
    "        p_ij = self.calc_pij(X_dists)\n",
    "        p_ij = p_ij[~np.eye(N, dtype=bool)].reshape(N, -1)\n",
    "\n",
    "        Y = np.random.random((N, self.n_components))\n",
    "\n",
    "        best_i, best_C = -1, np.inf\n",
    "        self.C_history = []\n",
    "        for i in tqdm(range(self.n_iter), disable=not self.verbose, desc=\"iteration\"):\n",
    "            Y_diff = Y[:, np.newaxis, :] - Y\n",
    "            # remove i=j cases\n",
    "            Y_diff = Y_diff[~np.eye(N, dtype=bool)].reshape(N, N - 1, -1)\n",
    "            Y_dists = np.linalg.norm(Y_diff, axis=-1)\n",
    "            Y_dists = 1 + Y_dists**2\n",
    "            Y_dists = Y_dists**-1\n",
    "            # normalize to a probabilty\n",
    "            q_ij = Y_dists / np.sum(Y_dists)\n",
    "\n",
    "            C = p_ij * np.log(p_ij / q_ij)\n",
    "            C = float(np.sum(C))\n",
    "            self.C_history.append(C)\n",
    "            if self.verbose:\n",
    "                print(f\"After {i} iterations, {C=}\")\n",
    "            if C < best_C:\n",
    "                best_i, best_C = i, C\n",
    "            if best_i + self.n_iter_without_progress < i:\n",
    "                if self.verbose:\n",
    "                    print(\n",
    "                        f\"C has not improved for {self.n_iter_without_progress} steps. Stopping early.\"\n",
    "                    )\n",
    "                return Y\n",
    "            # grad step\n",
    "            grad_C_yi = 4 * np.sum(\n",
    "                (p_ij - q_ij)[..., np.newaxis] * Y_diff * Y_dists[..., np.newaxis],\n",
    "                axis=1,\n",
    "            )\n",
    "            Y -= self.learning_rate * grad_C_yi\n",
    "        if self.verbose:\n",
    "            print(f\"{C=} after {self.n_iter} iterations\")\n",
    "        # Return Y, the 2D representation of the input data\n",
    "        return Y\n",
    "\n",
    "    # Part 2: Transformation of New Data Points\n",
    "    def transform(self, X_original, Y_original, X_new):\n",
    "        N = len(X_original)\n",
    "        n_neighbors = min(N, int(self.transform_n_neighbors))\n",
    "        Y_new = []\n",
    "\n",
    "        # Added weights to the distances as following: [1, 1/2, 1/3, 1/4.... 1/n]\n",
    "        # The minimal distance get's the highest weight, and the weight is reduced as the distance increasing\n",
    "        W_arr = np.arange(1, n_neighbors + 1, dtype=float) ** -1\n",
    "\n",
    "        # Calculate the distance array for each one of the test values\n",
    "        dists = cdist(X_new, X_original)\n",
    "\n",
    "        # Sort the indices of the distances from minimal to maximal while limit just to the n_neighbors distances\n",
    "        n_nearest_indexes = np.argsort(dists, axis=1)[:, :n_neighbors]\n",
    "\n",
    "        # Find the average location based on the indices of the distances, while the distances are weighted (Minimal distance -> max weight)\n",
    "        Y_new = np.average(Y_original[n_nearest_indexes], weights=W_arr, axis=1)\n",
    "\n",
    "        return np.array(Y_new)\n",
    "\n",
    "    def calc_pij(self, X_dists):\n",
    "        N = len(X_dists)\n",
    "        min_sigma = np.ones((N, 1)) * self.min_sigma\n",
    "        max_sigma = np.ones((N, 1)) * self.max_sigma\n",
    "        # precision is min_sigma / 10\n",
    "        n_iters = int(np.ceil(np.log2(self.max_sigma / (self.min_sigma / 10))))\n",
    "        for _ in tqdm(range(n_iters), disable=not self.verbose, desc=\"finding sigma\"):\n",
    "            curr_sigma = (min_sigma + max_sigma) / 2\n",
    "            p_ij = -(X_dists**2) / (2 * curr_sigma**2)\n",
    "            # remove i=j cases\n",
    "            p_ij = p_ij[~np.eye(N, dtype=bool)].reshape(N, -1)\n",
    "            # numeric stability in exponent\n",
    "            p_ij -= p_ij.max(axis=1, keepdims=True)\n",
    "            p_ij = np.exp(p_ij)\n",
    "            # normalize to a probabilty\n",
    "            p_ij = p_ij / p_ij.sum(axis=1, keepdims=True)\n",
    "            distribution_entropy = (-p_ij * np.log2(p_ij)).sum(axis=1, keepdims=True)\n",
    "            distribution_perplexity = 2**distribution_entropy\n",
    "            # update sigma\n",
    "            min_sigma, max_sigma = (\n",
    "                np.where(\n",
    "                    distribution_perplexity < self.perplexity, curr_sigma, min_sigma\n",
    "                ),\n",
    "                np.where(\n",
    "                    distribution_perplexity > self.perplexity, curr_sigma, max_sigma\n",
    "                ),\n",
    "            )\n",
    "        # reconstruct diagonal\n",
    "        p_ij = np.insert(p_ij, np.arange(0, N**2, N), 0).reshape((N, N))\n",
    "        return (p_ij + p_ij.T) / (2 * N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24f179351fa008",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load data\n",
    "Please use the cell below to discuss your dataset choice and why it is appropriate (or not) for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4083f-5267-44d3-89ed-65864f82aa57",
   "metadata": {},
   "source": [
    "We chose to use MNIST dataset to our algorithm.\n",
    "MNIST is a very popular yet simple dataset, which is used wordlwide for basic/begginer ML tasks, and has many examples specifically for dimensionality reduction outputs (PCA and tSNE). This will allow us to compare ourself. \n",
    "It's usage is about encoding a digit in an image (Assuming the digit is shown as pixels just like one takes a photo of a car plate), and each image is labeled by it's digit value\n",
    "\n",
    "Using T-SNE algorithm, we can find the relationship and the distances between the images with a very clear and visual way, while since we have just 10 digits in MNIST (0-9), we can classify the images into 10 groups and show in 2D dimension the relationship and distance between them\n",
    "\n",
    "We split the dataset into train (80%) and test (20%) while the dataset contains 10,000 examples in total\n",
    "\n",
    "Since the image values are described by grayscale pixel codes (0 -> white to 255 -> black), the digits should be represented as black while the background as white, but in fact the digits are in various range of colors between white and black (gray, dark gray. etc). Then, in order to reduce errors since the colors distribution, we want to normalize it so the digit will be represented with one single pixel code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a3b8890e86f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Loading the data into train / test datasets\n",
    "N_samples = 1000\n",
    "dataset = pd.read_csv(\"./Dataset/mnist_dataset.csv\")[:N_samples]\n",
    "\n",
    "# The X axis is the pixels grayscale matrix that represents the digit\n",
    "X = dataset.loc[:, \"1x1\":].values\n",
    "\n",
    "# The y axis is the actual digit that the the pixels matrix represents\n",
    "y = dataset.loc[:, \"label\"].values\n",
    "\n",
    "X_train, X_test, label_train, label_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Normalizing the grayscale codes by minmax scaler.\n",
    "sc = MinMaxScaler()\n",
    "X_train_normalized_opt1 = sc.fit_transform(X_train)\n",
    "X_test_normalized_opt1 = sc.transform(X_test)\n",
    "\n",
    "# Normalizing the grayscale codes by standard scaler\n",
    "sc = StandardScaler()\n",
    "X_train_normalized_opt2 = sc.fit_transform(X_train)\n",
    "X_test_normalized_opt2 = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061fa218-e946-419f-af5f-c26a204b669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14321076-8d5c-4f75-9eec-a123d4802b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda1548-1b9f-4b90-92e5-2675c8a0444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized_opt1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377574fd-827f-4685-a046-e2f40a078a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a ploting of sample train data\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    4,\n",
    "    10,\n",
    "    figsize=(10, 4),\n",
    "    subplot_kw={\"xticks\": [], \"yticks\": []},\n",
    "    gridspec_kw=dict(hspace=0.1, wspace=0.1),\n",
    ")\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(\n",
    "        X_train_normalized_opt1[i].reshape(28, 28),\n",
    "        cmap=\"binary\",\n",
    "        interpolation=\"nearest\",\n",
    "        clim=(0, 16),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b97ea-6e25-4620-9db0-3e150749729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a ploting of sample train data\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    4,\n",
    "    10,\n",
    "    figsize=(10, 4),\n",
    "    subplot_kw={\"xticks\": [], \"yticks\": []},\n",
    "    gridspec_kw=dict(hspace=0.1, wspace=0.1),\n",
    ")\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(\n",
    "        X_train_normalized_opt2[i].reshape(28, 28),\n",
    "        cmap=\"binary\",\n",
    "        interpolation=\"nearest\",\n",
    "        clim=(0, 16),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49bb42f79a55f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# t-SNE demonstration \n",
    "Demonstrate your t-SNE implementation.\n",
    "\n",
    "Add plots and figures. The code below is just to help you get started, and should not be your final submission.\n",
    "\n",
    "Please use the cell below to describe your results and tests.\n",
    "\n",
    "Describe the difference between your implementation and the sklearn implementation. Hint: you can look at the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064afb5-aeea-48d8-b315-921bf4f8238f",
   "metadata": {},
   "source": [
    "As we can see below, we've tested the module with a various combination of Hypher-Parameters. We ran everything on $N=1000$ MNIST examples, enough to see trends and still run fast at $O(N^2)$ time.\n",
    "\n",
    "The most important hypher-param is the perplexity. We found that a large number of perplexity causes a very dense and unified groups of digits but with some little mistakes since with high perplexity we can also \"catch\" neighbors that represents another disits. However, with a low number of perplexity the observabilities will not be unified and scattered along the graph\n",
    "Therefore, according to our research the optimal perplexity number is something about 30-50.\n",
    "\n",
    "Regarding learning rate, higher learning rate causes the group be closer to each other, and smaller learning rate makes the groups be more far away, since the Y output values are getting more centralized with higher learning rate value. The optimal learning rate is about 100-200. However, it has less affect on the results than the perplexity, and affects mostly the runtime.\n",
    "\n",
    "Redarding the number of iterations, we've tested with 1000 and 500 iterations and didn't find meaningful changes. This is also expressed in the loss over time graphs, which show that the loss decreases fast early on then slower as it reaches smaller values.\n",
    "It is important to note that a large number of iterations can cause overfitting (if we didn't implement early stopping), and smaller one can cause underfitting.\n",
    "\n",
    "Therefore, according to our experiments, we've found that the best hyper-parameters combination is as follows:\n",
    "* Perplexity of 50 (As previously told, small enough to not catch neigbors from another digits and large enough to make a unified and clear groups)\n",
    "* Learning rate of 200 (In order to make the groups unified and clear)\n",
    "* 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3628856e1335fd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hypher_pharams_options = [\n",
    "    {\"n_components\": 2, \"n_iter\": 1000, \"perplexity\": 100, \"learning_rate\": 200},\n",
    "    {\"n_components\": 2, \"n_iter\": 1000, \"perplexity\": 50, \"learning_rate\": 200},\n",
    "    {\"n_components\": 2, \"n_iter\": 1000, \"perplexity\": 30, \"learning_rate\": 100},\n",
    "    {\"n_components\": 2, \"n_iter\": 500, \"perplexity\": 30, \"learning_rate\": 100},\n",
    "    {\"n_components\": 2, \"n_iter\": 1000, \"perplexity\": 15, \"learning_rate\": 50},\n",
    "    {\"n_components\": 2, \"n_iter\": 1000, \"perplexity\": 50, \"learning_rate\": 200},\n",
    "    {\"n_components\": 2, \"n_iter\": 1000, \"perplexity\": 5, \"learning_rate\": 50},\n",
    "]\n",
    "\n",
    "best_hypher_pharams_options = [\n",
    "    {\"n_components\": 2, \"n_iter\": 1000, \"perplexity\": 50, \"learning_rate\": 200}\n",
    "]\n",
    "\n",
    "results_array = []\n",
    "\n",
    "for hypher_params in hypher_pharams_options:\n",
    "    n_components = hypher_params[\"n_components\"]\n",
    "    n_iter = hypher_params[\"n_iter\"]\n",
    "    perplexity = hypher_params[\"perplexity\"]\n",
    "    learning_rate = hypher_params[\"learning_rate\"]\n",
    "    print(\n",
    "        f\"Running with number of components {n_components}, number of iterations {n_iter}, perplexity {perplexity} and lr {learning_rate}\"\n",
    "    )\n",
    "\n",
    "    # Run your custom t-SNE implementation\n",
    "    custom_tsne = CustomTSNE(\n",
    "        n_components=n_components,\n",
    "        n_iter=n_iter,\n",
    "        perplexity=perplexity,\n",
    "        learning_rate=learning_rate,\n",
    "    )\n",
    "    custom_Y = custom_tsne.fit_transform(X_train_normalized_opt1)\n",
    "\n",
    "    # Run sklearn t-SNE\n",
    "    sk_tsne = TSNE(n_components=n_components, init=\"random\", perplexity=perplexity)\n",
    "    sk_Y = sk_tsne.fit_transform(X_train_normalized_opt1)\n",
    "\n",
    "    results_array.append(\n",
    "        {\n",
    "            \"custom_Y\": custom_Y,\n",
    "            \"sk_Y\": sk_Y,\n",
    "            \"custom_kl\": custom_tsne.C_history,\n",
    "            \"sk_kl\": sk_tsne.kl_divergence_,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd6fc4-a435-4e9d-b1bf-5d4015d7030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, result in enumerate(results_array):\n",
    "    hyperparams_string = f\"{hypher_pharams_options[idx]['n_iter']} iterations, perplexity {hypher_pharams_options[idx]['perplexity']} and lr {hypher_pharams_options[idx]['learning_rate']}\"\n",
    "    # Visualization of the result\n",
    "    plt.figure()\n",
    "    plt.scatter(\n",
    "        result[\"custom_Y\"][:, 0],\n",
    "        result[\"custom_Y\"][:, 1],\n",
    "        s=5,\n",
    "        c=label_train.astype(int),\n",
    "        cmap=\"tab10\",\n",
    "    )\n",
    "    plt.colorbar()\n",
    "    plt.title(\n",
    "        f\"MNIST Data Embedded into 2D with Custom t-SNE with {hyperparams_string}\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(\n",
    "        result[\"sk_Y\"][:, 0],\n",
    "        result[\"sk_Y\"][:, 1],\n",
    "        s=5,\n",
    "        c=label_train.astype(int),\n",
    "        cmap=\"tab10\",\n",
    "    )\n",
    "    plt.colorbar()\n",
    "    plt.title(\n",
    "        f\"MNIST Data Embedded into 2D with Sklearn t-SNE with {hyperparams_string}\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(\n",
    "        np.arange(len(result[\"custom_kl\"])),\n",
    "        result[\"custom_kl\"],\n",
    "        label=\"custom KL divergence over time\",\n",
    "    )\n",
    "    plt.axhline(\n",
    "        y=result[\"sk_kl\"], label=\"sk final KL divergence\", color=\"green\", linestyle=\"--\"\n",
    "    )\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"KL divergence function over time with {hyperparams_string}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5b80c",
   "metadata": {},
   "source": [
    "In terms of visual results, both outputs look good - we have clear seperation of different digits.\n",
    "The quality of the embeddings is also good - the loss decreases along the learning process, and the final KL divergence is always close between the implementations - it depends on the hyperparams of course, but we did not see any major preference twords one implementation over the that note subject.\n",
    "\n",
    "As noted before, the most important difference between our and sk-learn's implementation is the running time. Ours runs in $O(N^2)$ while sk's runs in $O(N \\log{N})$ which is way better especially as $N$ grows larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa2fceedc77e92",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# t-SNE extension - mapping new samples\n",
    "Demonstrate your t-SNE transformation procedure.\n",
    "\n",
    "Add plots and figures.\n",
    "\n",
    "Please use the cell below t describe your suggested approach in detail. Use formal notations where appropriate.\n",
    "Describe and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493f09c-41a0-416e-935d-eddd6673959a",
   "metadata": {},
   "source": [
    "We chose to use the transform method as following:\n",
    "1. For each one of the test set observabilities, we've calculated the distance from it's grayscale vector to any of the grayscale vectors in the train set\n",
    "2. We find the top n nearest vectors (While n is the perplexity used in the fit_transofrm method) and we build a new matrix of (n X 2) that contains the original Y values of the n nearest vectors from the train set, sorted from 1st nearest to n nearest (Each Y value contains 2 values, the x axis and the y axis in the target 2D graph)\n",
    "3. The target Y value of the test grayscale vector is the weighted average of the Y values of the n nearest vectors from the train set. While the weight of each value is $\\frac{1}{i}, 1 \\leq i \\leq n$ (The nearest value is weighted by $1$, the 2nd by $\\frac{1}{2}$, the 3rd by $\\frac{1}{3}$, and the n by $\\frac{1}{n}$)\n",
    "4. We repeat this for each one of the test values, until we build a final Y vector of shape $(N_{test}, 2)$ representing 2D values to be in the graph.\n",
    "\n",
    "The reason we compare the distances between the grayscale vectors of each one of the test examples to the train examples, is the assumption that each digit represented by a pixel code is not exactly the same as other pixel code vectors of the same digit, but the distance is minimal to grayscale vectors represents the same digit.\n",
    "Then we assume that most of the n grayscale nearest vectors represents the same digit as the test sample.\n",
    "\n",
    "Then we want to locate it around the n grayscale nearest vectors but with more weight to the closest vector and less weight as the vector is getting less close.\n",
    "\n",
    "With that approach we set the Y vectors of the test samples around the group of their digits.\n",
    "\n",
    "In order to measure the performance of each one of the examples, I've added two dataframe tables for each one of the experiments:\n",
    "1. The first table compares each test observability actual digit with it's 10 closets train actual digits (As more neighbors have the same digit as the test observability, then the experiment is better)\n",
    "2. The second tables measures the 10 closest train distances to each one of the test observabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38dc132b23e7b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for idx, result in enumerate(results_array):\n",
    "    # Transform new data\n",
    "    custom_Y_new = custom_tsne.transform(\n",
    "        X_train_normalized_opt1, result[\"custom_Y\"], X_test\n",
    "    )\n",
    "\n",
    "    # Visualization of the result\n",
    "    plt.figure()\n",
    "    plt.scatter(\n",
    "        result[\"custom_Y\"][:, 0],\n",
    "        result[\"custom_Y\"][:, 1],\n",
    "        s=5,\n",
    "        c=label_train.astype(int),\n",
    "        cmap=\"tab10\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        custom_Y_new[:, 0],\n",
    "        custom_Y_new[:, 1],\n",
    "        marker=\"*\",\n",
    "        s=50,\n",
    "        linewidths=0.5,\n",
    "        edgecolors=\"k\",\n",
    "        c=label_test.astype(int),\n",
    "        cmap=\"tab10\",\n",
    "    )\n",
    "    plt.colorbar()\n",
    "    plt.title(\n",
    "        f\"MNIST Data Embedded into 2D with Custom t-SNE with {hypher_pharams_options[idx]['n_iter']} iterations, perplexity {hypher_pharams_options[idx]['perplexity']} and lr {hypher_pharams_options[idx]['learning_rate']}\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # Top 10 nearest neighbor of each test set\n",
    "    dists = cdist(custom_Y_new, result[\"custom_Y\"])\n",
    "    n_nearest_digits = label_train[np.argsort(dists, axis=1)[:, :10]]\n",
    "    n_nearest_distances = np.sort(dists, axis=1)[:, :10]\n",
    "\n",
    "    df1 = pd.DataFrame(label_test, columns=[\"actual digit\"])\n",
    "    df2 = pd.DataFrame(\n",
    "        n_nearest_digits,\n",
    "        columns=[f\"{idx+1}th closest digit\" for idx in range(n_nearest_digits.shape[1])],\n",
    "    )\n",
    "    df3 = pd.DataFrame(\n",
    "        n_nearest_distances,\n",
    "        columns=[\n",
    "            f\"{idx+1}th closest distance\" for idx in range(n_nearest_distances.shape[1])\n",
    "        ],\n",
    "    )\n",
    "    df_digits = df1.join(df2)\n",
    "    df_distances = df1.join(df3)\n",
    "\n",
    "    most_common_neigbors_digits = np.array(\n",
    "        [np.bincount(row).argmax() for row in n_nearest_digits]\n",
    "    )\n",
    "    digits_matrix = confusion_matrix(label_test, most_common_neigbors_digits)\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix=digits_matrix)\n",
    "    cm_display.plot(cmap=\"magma\")\n",
    "    plt.title(\n",
    "        \"Actual test digit vs the most common digit in the nearest 10 train neighbors\"\n",
    "    )\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "    df_measures = pd.DataFrame(\n",
    "        [\n",
    "            [\n",
    "                precision_score(\n",
    "                    label_test, most_common_neigbors_digits, average=\"weighted\"\n",
    "                ),\n",
    "                recall_score(\n",
    "                    label_test, most_common_neigbors_digits, average=\"weighted\"\n",
    "                ),\n",
    "            ]\n",
    "        ],\n",
    "        columns=[\"precision\", \"recall\"],\n",
    "    )\n",
    "    display(df_measures)\n",
    "\n",
    "    print(\"10 Closests train actual digits for each test digit\")\n",
    "    display(df_digits[:20])\n",
    "    print(\"10 Closests train distances for each test oservability\")\n",
    "    display(df_distances[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c95c7f-d3a9-4e3d-b539-02e020358766",
   "metadata": {},
   "source": [
    "# Use of generative AI\n",
    "Please use the cell below to describe your use of generative AI in this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36753fd7-8b2d-487b-82ae-dc318eca3ca6",
   "metadata": {},
   "source": [
    "We used ChatGPT to help us utilize numpy vectorised functions more effieintly. It also proposed subtracting the maximum when calculating exponents, in order to avoid very large/small values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee67dc6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
